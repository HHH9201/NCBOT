# -*- coding: utf-8 -*-
"""
å’¸é±¼å•æœºï¼ˆå•æœºç‰ˆï¼‰+ ByrutGameï¼ˆè”æœºç‰ˆï¼‰
åŒåŒ¿ååˆå¹¶è½¬å‘å¡ç‰‡
"""
import re
import os
import json
import asyncio
import logging
import yaml
import string
import base64
import aiohttp
import aiofiles
import requests
import urllib3
from pathlib import Path
from bs4 import BeautifulSoup
from urllib.parse import urljoin
from ncatbot.plugin import BasePlugin, CompatibleEnrollment
from ncatbot.core.message import GroupMessage
from ncatbot.core import Text, At, Reply, MessageChain, Image

# é…ç½®æ›´æ¸…çˆ½çš„æ—¥å¿—æ ¼å¼ï¼Œå»æ‰è¿›ç¨‹å’Œçº¿ç¨‹ä¿¡æ¯

# APIé…ç½®
API_URL = "https://api.siliconflow.cn/v1/chat/completions"
API_HEADERS = {
    "Authorization": "Bearer sk-ixmsswryqnmuyifjewdetqnjewdetq",
    "Content-Type": "application/json"
}

bot = CompatibleEnrollment


# -------------------- åŸºç¡€é…ç½® --------------------
QQ_IMG = "/home/hjh/BOT/NCBOT/plugins/xydj/tool/TG.png"
HEADERS = {"User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36"}
COOKIES = {
    "_ok4_": "k7Ss53TvUyeXcsWfWBuG3EDFCuhFBobWvQAsWPR4u7n/Fx1oJgJ582qBY9G6J2s2mjP6qe3nvbV2HpnvLzDf8bo1kDTJie9uhaXfKSSh1qmvZC0OvV41h5ex++Iw3moO",
    "ripro_notice_cookie": "1",
    "TDC_itoken": "619377261%3A1765550565",
    "PHPSESSID": "10ul6aj41hfuguh3j71fvvn4g4",
    "wordpress_logged_in_c1baf48ff9d49282e5cd4050fece6d34": "HHH9201%7C1766760171%7C5mAlNcIYZrvtS4Cxg5ckAeyivEqEGcYux4YtHtolbnS%7Cb012109b8803e7f36bc7bd9cf4f0a475a7dd72465b8f78c60e7c228c698f3316"
}
PROXY = "http://127.0.0.1:7899"
BYRUT_BASE = "https://napcat.1783069903.workers.dev"
session = requests.Session()
session.headers.update(HEADERS)
session.proxies.update({"http": PROXY, "https": PROXY})
session.verify = False
urllib3.disable_warnings()

CACHE_FILE = Path(__file__).parent / "game_name_cache.yaml"

# -------------------- å·¥å…·å‡½æ•° --------------------
def load_cache():
    try:
        if CACHE_FILE.exists():
            with open(CACHE_FILE, 'r', encoding='utf-8') as f:
                return yaml.safe_load(f) or {}
    except Exception as e:
        logging.warning(f"åŠ è½½ç¼“å­˜å¤±è´¥: {e}")
    return {}

def save_cache(c):
    try:
        with open(CACHE_FILE, 'w', encoding='utf-8') as f:
            yaml.dump(c, f, allow_unicode=True, sort_keys=True)
    except Exception as e:
        logging.error(f"ä¿å­˜ç¼“å­˜å¤±è´¥: {e}")

def normalize(txt):
    for p in string.punctuation:
        txt = txt.replace(p, " ")
    return " ".join(txt.lower().split())

# ------------------------------------------------------------------
# 2. æŠŠè‹±æ–‡å…³é”®è¯ç¿»è¯‘æˆä¸­æ–‡å®˜æ–¹åï¼ˆä¾›ç•Œé¢å±•ç¤ºï¼Œä¸ç”¨äºæœç´¢ï¼‰
# ------------------------------------------------------------------
_title_cache = {}          # é‡å¯å³å¤±æ•ˆçš„å†…å­˜ç¼“å­˜ï¼Œå¦‚éœ€æŒä¹…åŒ–å¯æ”¹ redis

def translate_to_chinese_title(eng: str) -> str:
    """
    è¾“å…¥è‹±æ–‡å…³é”®è¯ï¼Œè¿”å› Steam å®˜æ–¹ä¸­æ–‡åï¼›å¤±è´¥åˆ™å›é€€åŸæ–‡ã€‚
    ç¼“å­˜ 1 å°æ—¶ï¼Œé¿å…é‡å¤è¯·æ±‚ã€‚
    """
    if not eng:
        return  eng

    global  _title_cache
    if eng in _title_cache:
        return _title_cache[eng]

    payload = {
        "model": "moonshotai/Kimi-K2-Instruct-0905",
        "messages": [
            {"role": "system", "content": "ä½ æ˜¯ Steam ä¸­æ–‡åç§°ç¿»è¯‘åŠ©æ‰‹ï¼Œåªè¾“å‡º steam æ¸¸æˆå®˜æ–¹ä¸­æ–‡åï¼Œå…¶ä½™ä»»ä½•æ–‡å­—éƒ½ä¸è¦è¯´ã€‚"},
            {"role": "user", "content": f"{eng} çš„ Steam å®˜æ–¹æ¸¸æˆä¸­æ–‡åæ˜¯ä»€ä¹ˆ"}
        ],
        "temperature": 0.1,
        "max_tokens": 30
    }
    try:
        resp = requests.post(API_URL, json=payload, headers=API_HEADERS,
                             proxies=PROXY, timeout=10)
        resp.raise_for_status()
        zh = resp.json()["choices"][0]["message"]["content"].strip()
    except Exception as e:
        logging.warning("ä¸­æ–‡ç¿»è¯‘å¤±è´¥: %s", e)
        zh = eng          # å¤±è´¥å°±å›é€€åŸæ–‡

    _title_cache[eng] =  zh
    return zh

# ------------------------------------------------------------------
# 1. æå–è‹±æ–‡å…³é”®è¯ + ä¸­æ–‡å±•ç¤ºåï¼ˆè¿”å› tupleï¼‰
# ------------------------------------------------------------------
def extract_english_name(title: str) -> tuple[str, str]:

    segments = title.split('|')
    
    # å¯»æ‰¾æœ€ç®€æ´çš„è‹±æ–‡æ¸¸æˆå
    english_part = ""
    for segment in reversed(segments):  # ä»åå¾€å‰æ‰¾è‹±æ–‡æ®µ
        segment = segment.strip()
        # å¦‚æœè¿™æ®µä¸»è¦æ˜¯è‹±æ–‡å­—ç¬¦ï¼Œå°±è®¤ä¸ºæ˜¯è‹±æ–‡æ®µ
        if len(re.findall(r'[a-zA-Z]', segment)) > len(re.findall(r'[\u4e00-\u9fff]', segment)):
            english_part = segment
            break
    
    # å¦‚æœæ²¡æ‰¾åˆ°è‹±æ–‡æ®µï¼Œç”¨æœ€åä¸€æ®µ
    if not english_part:
        english_part = segments[-1] if segments else title
    
    # ä¸­æ–‡å±•ç¤ºåï¼šä»ç¬¬ä¸€ä¸ªä¸­æ–‡æ®µå¼€å§‹ï¼Œåˆ°è‹±æ–‡æ®µä¹‹å‰çš„æ‰€æœ‰æ®µ
    chinese_display_parts = []
    for segment in segments:
        segment = segment.strip()
        # å¦‚æœè¿™æ®µä¸»è¦æ˜¯è‹±æ–‡å­—ç¬¦ï¼Œå°±åœæ­¢æ”¶é›†
        if len(re.findall(r'[a-zA-Z]', segment)) > len(re.findall(r'[\u4e00-\u9fff]', segment)):
            break
        chinese_display_parts.append(segment)
    
    # å¦‚æœæ²¡æ‰¾åˆ°ä¸­æ–‡æ®µï¼Œç”¨ç¬¬ä¸€æ®µ
    if not chinese_display_parts:
        chinese_display_parts = [segments[0]] if segments else [title]
    
    chinese_display = ' | '.join(chinese_display_parts)
    
    # æ¸…ç†è‹±æ–‡éƒ¨åˆ†ï¼šå»æ‰ç‰ˆæœ¬å·ã€å¹´ä»½ã€ç‰¹æ®Šç¬¦å·ç­‰
    # å»æ‰æ‹¬å·åŠå…¶å†…å®¹
    english_part = re.sub(r'\([^)]*\)', '', english_part)
    english_part = re.sub(r'\[[^\]]*\]', '', english_part)
    # å»æ‰æ–œæ åçš„é‡å¤å†…å®¹
    english_part = english_part.split('/')[0]
    # å»æ‰ç‰¹æ®Šç¬¦å·å’Œå¤šä½™ç©ºæ ¼
    english_part = re.sub(r'[^\w\s]', ' ', english_part)
    english_part = re.sub(r'\s+', ' ', english_part).strip()
    
    # åªä¿ç•™å‰3-4ä¸ªæ ¸å¿ƒå•è¯
    words = english_part.split()
    if len(words) > 4:
        english_part = ' '.join(words[:4])
    
    # ç½—é©¬â†’é˜¿æ‹‰ä¼¯æ•°å­—ï¼ˆä»…è‹±æ–‡å…³é”®è¯ï¼‰
    roman_to_arabic = {
        'I': '1', 'II': '2', 'III': '3', 'IV': '4', 'V': '5',
        'VI': '6', 'VII': '7', 'VIII': '8', 'IX': '9', 'X': '10',
        'XI': '11', 'XII': '12', 'XIII': '13', 'XIV': '14', 'XV': '15',
        'XVI': '16', 'XVII': '17', 'XVIII': '18', 'XIX': '19', 'XX': '20'
    }
    # æ•´è¯æ›¿æ¢ï¼Œå¿½ç•¥å¤§å°å†™
    for roman, arabic in roman_to_arabic.items():
        english_part = re.sub(rf'\b{roman}\b', arabic, english_part, flags=re.I)
    
    return english_part.strip(), chinese_display.strip()

# åˆ é™¤ get_text_size å‡½æ•°ï¼Œä¸å†ä½¿ç”¨

async def fetch_text(url, **kwargs):
    async with aiohttp.ClientSession(cookies=COOKIES, headers=HEADERS) as session:
        async with session.get(url, **kwargs) as resp:
            return await resp.text()

async def get_real_url(jump_url: str) -> str:
    async with aiohttp.ClientSession(cookies=COOKIES, headers=HEADERS) as s:
        async with s.head(jump_url, allow_redirects=False) as r:
            if 300 <= r.status < 400:
                return r.headers['Location']
        async with s.get(jump_url) as r:
            return str(r.url)

# -------------------- xydj æœç´¢ --------------------
async def search_game(game_name: str):
    url = f"https://www.xianyudanji.to/?cat=1&s={game_name}&order=views"
    html = await fetch_text(url, timeout=15)
    soup = BeautifulSoup(html, "lxml")
    games, seen = [], set()
    for a in soup.select("article.post-grid a[href][title]"):
        title = a['title'].strip()
        img_tag = a.select_one("img")
        img_src = img_tag['src'] if img_tag else ""
        if not title or title in seen or game_name not in title:
            continue
        seen.add(title)
        games.append({"title": title, "url": a['href'], "img": img_src})
    if not games:
        return None, None
    
    # ç›´æ¥è¿”å›æ–‡æœ¬æ ¼å¼çš„æ¸¸æˆåˆ—è¡¨ï¼Œä¸ç”Ÿæˆå›¾ç‰‡
    text_lines = []
    
    for idx, g in enumerate(games):
        # æå–æ¸¸æˆåå’Œç‰ˆæœ¬ä¿¡æ¯
        title_parts = g['title'].split('|')
        game_name = title_parts[0].strip()
        
        # æå–å…³é”®ä¿¡æ¯ï¼Œä¿æŒç®€æ´
        key_info = []
        for part in title_parts[1:]:
            part = part.strip()
            if any(keyword in part.lower() for keyword in ['v', 'ç‰ˆ', 'dlc', 'ä¸­æ–‡', 'æ‰‹æŸ„', 'æ›´æ–°', 'å¹´åº¦ç‰ˆ']):
                key_info.append(part)
        
        # æ„å»ºç¾è§‚çš„æ ¼å¼
        display_text = f"ğŸ”¹ {idx+1}. {game_name}"
        if key_info:
            display_text += f" | {' | '.join(key_info[:3])}"
        
        text_lines.append(display_text)
    
    text_result = "\n".join(text_lines)
    
    return text_result, games

# -------------------- xydj è¯¦æƒ… --------------------
async def extract_download_info(game_url: str):
    try:
        html = await fetch_text(game_url, timeout=15)
        soup = BeautifulSoup(html, "lxml")
        box = soup.select_one("#ripro_v2_shop_down-5")
        if not box:
            return ["æœªæ‰¾åˆ°ä¸‹è½½åŒºåŸŸ"]
        results = []
        
        # æå–è§£å‹å¯†ç  - æ”¯æŒä¸¤ç§ä¸åŒçš„HTMLæ ¼å¼
        password_found = False
        
        # æ–¹æ³•1: ä»æŒ‰é’®ç»„ä¸­æå–è§£å‹å¯†ç ï¼ˆç¬¬ä¸€ç§æ ¼å¼ï¼‰
        # æŸ¥æ‰¾æŒ‰é’®ç»„ä¸­åŒ…å«"è§£å‹å¯†ç "æ–‡æœ¬çš„æŒ‰é’®
        password_btns = box.select('div.btn-group button.go-copy[data-clipboard-text]')
        for btn in password_btns:
            btn_text = btn.get_text(strip=True)
            # æ£€æŸ¥æŒ‰é’®æ–‡æœ¬æˆ–ç›¸é‚»çš„é“¾æ¥æ–‡æœ¬æ˜¯å¦åŒ…å«"è§£å‹å¯†ç "
            adjacent_link = btn.find_previous_sibling('a') if btn else None
            link_text = adjacent_link.get_text(strip=True) if adjacent_link else ""
            
            if ('è§£å‹å¯†ç ' in btn_text or 'è§£å‹å¯†ç ' in link_text):
                        clipboard_text = btn.get('data-clipboard-text', '').strip()
                        if clipboard_text:  # ç¡®ä¿å¯†ç ä¸ä¸ºç©º
                            results.append(f"è§£å‹å¯†ç : ã€{clipboard_text}ã€‘")
                            password_found = True
                            break
        
        # æ–¹æ³•2: ä»down-infoåŒºåŸŸæå–è§£å‹å¯†ç ï¼ˆç¬¬äºŒç§æ ¼å¼ï¼‰
        if not password_found:
            down_info = box.select_one('div.down-info')
            if down_info:
                # æŸ¥æ‰¾åŒ…å«"è§£å‹å¯†ç "çš„liå…ƒç´ 
                password_lis = down_info.select('ul.infos li')
                for li in password_lis:
                    data_label = li.select_one('p.data-label')
                    if data_label and 'è§£å‹å¯†ç ' in data_label.get_text():
                        info_p = li.select_one('p.info')
                        if info_p:
                            # æå–å¯†ç æ–‡æœ¬ï¼Œå¯èƒ½åŒ…å«åœ¨spanæˆ–bæ ‡ç­¾å†…
                            password_span = info_p.select_one('span')
                            password_b = info_p.select_one('b')
                            
                            if password_span:
                                password = password_span.get_text(strip=True)
                            elif password_b:
                                password = password_b.get_text(strip=True)
                            else:
                                password = info_p.get_text(strip=True)
                            
                            # éªŒè¯å¯†ç æ ¼å¼å¹¶æ·»åŠ åˆ°ç»“æœ
                            if password and password != "è§£å‹å¯†ç =å®‰è£…å¯†ç ã€æ¿€æ´»ç ":  # æ’é™¤è¯´æ˜æ–‡å­—
                                results.append(f"è§£å‹å¯†ç : ã€{password}ã€‘")
                                password_found = True
                                break
        
        # æ–¹æ³•3: é€šç”¨å¤‡ç”¨æ–¹æ¡ˆ - æŸ¥æ‰¾ä»»ä½•å¯èƒ½åŒ…å«å¯†ç çš„å…ƒç´ 
        if not password_found:
            # æŸ¥æ‰¾æ‰€æœ‰å¯èƒ½åŒ…å«å¯†ç çš„å…ƒç´ 
            potential_password_elements = box.select('[data-clipboard-text]')
            for element in potential_password_elements:
                clipboard_text = element.get('data-clipboard-text', '').strip()
                element_text = element.get_text(strip=True)
                
                # åˆ¤æ–­æ˜¯å¦ä¸ºæœ‰æ•ˆå¯†ç æ ¼å¼
                if (clipboard_text and 
                        len(clipboard_text) >= 4 and 
                        not any(keyword in clipboard_text for keyword in ['ç™¾åº¦', 'ç½‘ç›˜', 'æå–', 'https', 'http']) and
                        ('å¯†ç ' in element_text or 'è§£å‹' in element_text)):
                        results.append(f"è§£å‹å¯†ç : ã€{clipboard_text}ã€‘")
                        password_found = True
                        break
        
        # å¦‚æœæ‰€æœ‰æ–¹æ³•éƒ½å¤±è´¥äº†
        if not password_found:
            results.append("è§£å‹å¯†ç : æœªæ‰¾åˆ°")
        
        # æå–ç™¾åº¦ç½‘ç›˜æå–ç 
        bdpan_btn = None
        btn_groups = box.select('div.btn-group')
        if btn_groups:
            # æŸ¥æ‰¾åŒ…å«"ç™¾åº¦ç½‘ç›˜"çš„æŒ‰é’®ç»„
            for group in btn_groups:
                a_tag = group.select_one('a[href*="goto?down="]')
                if a_tag and 'ç™¾åº¦ç½‘ç›˜' in a_tag.get_text():
                    bdpan_btn = group.select_one('button.go-copy[data-clipboard-text]')
                    break
        
        if bdpan_btn and bdpan_btn.has_attr('data-clipboard-text'):
            results.append(f"ç™¾åº¦ç½‘ç›˜æå–ç : {bdpan_btn['data-clipboard-text'].strip()}")
        else:
            results.append("ç™¾åº¦ç½‘ç›˜æå–ç : æœªæ‰¾åˆ°")
            
        # æå–ä¸‹è½½é“¾æ¥
        for a in box.select("a[target='_blank'][href*='goto?down=']"):
            name = a.get_text(strip=True)
            if 'è§£å‹å¯†ç ' in name:
                continue
            jump_url = urljoin(game_url, a['href'])
            real_url = await get_real_url(jump_url)
            results.append(f"{name}: {real_url}")
        return results
    except Exception as e:
        return [f"è§£ææ¸¸æˆä¿¡æ¯æ—¶å‡ºé”™: {e}"]

# -------------------- ç½‘ç»œè¯·æ±‚é…ç½®å’Œé”™è¯¯å¤„ç† ----------
# ä»£ç†é…ç½®æ£€æŸ¥
if PROXY and not PROXY.startswith(('http://', 'https://', 'socks4://', 'socks5://')):
    logging.warning(f"[Network] ä»£ç†é…ç½®æ ¼å¼å¯èƒ½ä¸æ­£ç¡®: {PROXY}")
    PROXY = None  # ç¦ç”¨å¯èƒ½æœ‰é—®é¢˜çš„ä»£ç†

# å¢å¼ºçš„è¯·æ±‚å¤´é…ç½®ï¼ˆç§»é™¤Brotliæ”¯æŒä»¥é¿å…è§£ç é”™è¯¯ï¼‰
# æ·»åŠ æ›´å®Œæ•´çš„è¯·æ±‚å¤´ä»¥æ¨¡æ‹ŸçœŸå®æµè§ˆå™¨ï¼Œé¿å…403é”™è¯¯
HEADERS.update({
    'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
    'Accept-Language': 'zh-CN,zh;q=0.9,en;q=0.8',
    'Accept-Encoding': 'gzip, deflate',  # ç§»é™¤br(brotli)æ”¯æŒ
    'Cache-Control': 'no-cache',
    'Pragma': 'no-cache',
    'Referer': "https://byrutgame.org",  # æ·»åŠ Refererå¤´
    'Origin': "https://byrutgame.org",  # æ·»åŠ Originå¤´
    'DNT': '1',  # ä¸è¿½è¸ªè¯·æ±‚
    'Connection': 'keep-alive',  # ä¿æŒè¿æ¥
    'Upgrade-Insecure-Requests': '1',  # å‡çº§ä¸å®‰å…¨è¯·æ±‚
    'Sec-Fetch-Dest': 'document',
    'Sec-Fetch-Mode': 'navigate',
    'Sec-Fetch-Site': 'same-origin',
    'Sec-Fetch-User': '?1'
})

# -------------------- ByrutGame æœç´¢ï¼ˆå¼‚æ­¥+ä»£ç†+SSL å…³é—­ï¼‰ ----------
async def search_byrut(name: str) -> list:
    """è¿”å› [{href, title, category}, ...] æœ€å¤š3æ¡"""
    params = {"do": "search", "subaction": "search", "story": name}
    url = "https://byrutgame.org/index.php"
    
    # é‡è¯•æœºåˆ¶é…ç½®
    max_retries = 3
    retry_delay = 2
    text = None  # ç”¨äºå­˜å‚¨æˆåŠŸè·å–çš„æ–‡æœ¬
    
    for attempt in range(max_retries):
        connector = aiohttp.TCPConnector(ssl=False, force_close=True)
        timeout = aiohttp.ClientTimeout(total=30, connect=10, sock_connect=10, sock_read=10)
        session = None
        
        try:
            # æ¯æ¬¡é‡è¯•éƒ½åˆ›å»ºå…¨æ–°çš„sessionå’Œconnectorï¼Œé¿å…session closedé—®é¢˜
            # ä¸ºäº†é¿å…403ï¼Œæˆ‘ä»¬ä¼ é€’ä¸€ä¸ªç©ºçš„cookieså­—å…¸ï¼Œè®©æœåŠ¡å™¨è®¤ä¸ºæˆ‘ä»¬æ¥å—cookies
            session = aiohttp.ClientSession(
                connector=connector, 
                timeout=timeout, 
                headers=HEADERS,
                cookies={}  # æ·»åŠ ç©ºcookiesï¼Œé¿å…æœåŠ¡å™¨æ‹’ç»æ— cookiesè¯·æ±‚
            )
            
            # æ„å»ºè¯·æ±‚å‚æ•°
            request_params = {"params": params, "timeout": timeout}
            if PROXY:
                request_params["proxy"] = PROXY
            
            # æ‰“å°è°ƒè¯•æ—¥å¿—
            full_url = f"{url}?{ '&'.join([f'{k}={v}' for k, v in params.items()]) }"
            logging.debug(f"[Byrut] å‘é€è¯·æ±‚ï¼š{full_url}")
            logging.debug(f"[Byrut] è¯·æ±‚headersï¼š{HEADERS}")
            logging.debug(f"[Byrut] è¯·æ±‚proxyï¼š{PROXY}")
            
            async with session.get(url, **request_params) as resp:
                # æ‰“å°å“åº”æ—¥å¿—
                logging.debug(f"[Byrut] å“åº”çŠ¶æ€ç ï¼š{resp.status}")
                logging.debug(f"[Byrut] å“åº”headersï¼š{dict(resp.headers)}")
                
                if resp.status == 403:
                    logging.warning(f"[Byrut] 403 Forbidden (å°è¯• {attempt + 1}/{max_retries})")
                    # å°è¯•è·å–é”™è¯¯å“åº”å†…å®¹ï¼Œåˆ†æ403åŸå› 
                    error_content = await resp.text()
                    logging.debug(f"[Byrut] 403é”™è¯¯å“åº”å†…å®¹ï¼š{error_content[:500]}...")
                    
                    # 403æ—¶å°è¯•æ›´æ¢User-Agent
                    if attempt < max_retries - 1:
                        # è½®æ¢User-Agent
                        import random
                        user_agents = [
                            "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
                            "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/119.0.0.0 Safari/537.36",
                            "Mozilla/5.0 (Macintosh; Intel Mac OS X 14_0) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
                            "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Firefox/121.0",
                            "Mozilla/5.0 (Macintosh; Intel Mac OS X 14.1) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/17.1 Safari/605.1.15"
                        ]
                        # åˆ›å»ºä¸´æ—¶headerså‰¯æœ¬ï¼Œè½®æ¢User-Agent
                        temp_headers = HEADERS.copy()
                        temp_headers['User-Agent'] = random.choice(user_agents)
                        session.headers.update(temp_headers)
                        logging.debug(f"[Byrut] å°è¯•æ›´æ¢User-Agent: {temp_headers['User-Agent']}")
                        
                        await asyncio.sleep(retry_delay * (attempt + 1))  # æŒ‡æ•°é€€é¿
                        continue
                    return []          # ç©º = æœªæ‰¾åˆ°
                elif resp.status != 200:
                    logging.warning(f"[Byrut] åä»£è¿”å›çŠ¶æ€ç ï¼š{resp.status} (å°è¯• {attempt + 1}/{max_retries})")
                    # å°è¯•è·å–é”™è¯¯å“åº”å†…å®¹
                    error_content = await resp.text()
                    logging.debug(f"[Byrut] é”™è¯¯å“åº”å†…å®¹ï¼š{error_content[:500]}...")
                    if attempt < max_retries - 1:
                        await asyncio.sleep(retry_delay)
                        continue
                    return []          # ç©º = æœªæ‰¾åˆ°
                text = await resp.text()
                logging.debug(f"[Byrut] æˆåŠŸè·å–å“åº”ï¼Œé•¿åº¦ï¼š{len(text)} å­—ç¬¦")
                break  # æˆåŠŸè·å–æ•°æ®ï¼Œè·³å‡ºé‡è¯•å¾ªç¯
                
        except aiohttp.ClientConnectorError as e:
            logging.error(f"[Byrut] è¿æ¥é”™è¯¯ (å°è¯• {attempt + 1}/{max_retries}): {e}")
            if attempt < max_retries - 1:
                await asyncio.sleep(retry_delay * (attempt + 1))  # æŒ‡æ•°é€€é¿
                continue
            # æœ€åä¸€æ¬¡å°è¯•å¤±è´¥ï¼Œè¿”å›ç©ºç»“æœ
            logging.error("[Byrut] æ‰€æœ‰é‡è¯•å°è¯•å¤±è´¥ï¼Œè¿”å›ç©ºç»“æœ")
            return []
        except asyncio.TimeoutError as e:
            logging.error(f"[Byrut] è¯·æ±‚è¶…æ—¶ (å°è¯• {attempt + 1}/{max_retries}): {e}")
            if attempt < max_retries - 1:
                await asyncio.sleep(retry_delay)
                continue
            return []
        except aiohttp.ClientPayloadError as e:
            logging.error(f"[Byrut] æ•°æ®ä¼ è¾“é”™è¯¯ (å°è¯• {attempt + 1}/{max_retries}): {e}")
            if attempt < max_retries - 1:
                await asyncio.sleep(retry_delay)
                continue
            return []
        except Exception as e:
            logging.exception(f"[Byrut] æœç´¢è¯·æ±‚å¤±è´¥ (å°è¯• {attempt + 1}/{max_retries}): {e}")
            if attempt < max_retries - 1:
                await asyncio.sleep(retry_delay)
                continue
            return []
        finally:
            # ç¡®ä¿sessionè¢«æ­£ç¡®å…³é—­
            if session and not session.closed:
                await session.close()
            # ç¡®ä¿connectorè¢«æ­£ç¡®å…³é—­
            if connector and not connector.closed:
                await connector.close()
    
    # å¦‚æœæ²¡æœ‰è·å–åˆ°æ–‡æœ¬ï¼Œè¿”å›ç©ºç»“æœ
    if text is None:
        return []

    soup = BeautifulSoup(text, "html.parser")
    key = normalize(name)
    results, seen = [], set()
    for a in soup.select("a.search_res"):
        href = a["href"]
        if "po-seti" not in href.lower():   # â† åªç•™è”æœº
            continue
        title_tag = a.select_one(".search_res_title")
        if not title_tag:
            continue
        title = title_tag.get_text(strip=True)
        if key not in normalize(title):
            continue
        if href in seen:
            continue
        seen.add(href)
        category = (
            "è”æœºç‰ˆ"
            if any(k in href.lower() for k in ["po-seti", "onlayn", "multiplayer"])
            else "å•æœºç‰ˆ"
        )
        results.append({"href": href, "title": title, "category": category})
    return results[:3]   # æœ€å¤š3æ¡


# -------------------- å¤‡ç”¨æ–¹æ¡ˆå‡½æ•° --------------------
def _apply_backup_solution(item: dict, error_type: str) -> None:
    """åº”ç”¨å¤‡ç”¨æ–¹æ¡ˆï¼Œå½“ä¸»APIä¸å¯ç”¨æ—¶æä¾›åŸºæœ¬åŠŸèƒ½"""
    logging.info(f"[Byrut] {error_type}ï¼Œåº”ç”¨å¤‡ç”¨æ–¹æ¡ˆ")
    
    # ä½¿ç”¨åŸå§‹é“¾æ¥ä½œä¸ºå¤‡ç”¨ä¸‹è½½é“¾æ¥
    backup_torrent_url = item.get('href', '')
    
    # æ£€æŸ¥å¤‡ç”¨å›¾ç‰‡æ˜¯å¦å­˜åœ¨
    backup_image = "/home/hjh/BOT/NCBOT/plugins/xydj/tool/ç§å­.png"
    if not os.path.exists(backup_image):
        # å¦‚æœæ–‡ä»¶ä¸å­˜åœ¨ï¼Œä½¿ç”¨æ–‡å­—æ ‡è¯†
        backup_image = None
        logging.warning(f"[Byrut] å¤‡ç”¨å›¾ç‰‡æ–‡ä»¶ä¸å­˜åœ¨: {backup_image}")
    
    # æ›´æ–°é¡¹ç›®ä¿¡æ¯
    item.update({
        "update_time": f"APIè¿æ¥å¤±è´¥ï¼Œä½¿ç”¨å¤‡ç”¨èµ„æº ({error_type})", 
        "torrent_url": backup_torrent_url,
        "backup_image": backup_image,
        "backup_mode": True  # æ ‡è®°ä¸ºå¤‡ç”¨æ¨¡å¼
    })

# -------------------- ByrutGame è¯¦æƒ…ï¼ˆå¼‚æ­¥+ä»£ç†+SSL å…³é—­ï¼‰ ----------
async def fetch_byrut_detail(item: dict) -> None:
    href = item["href"]
    # æ£€æŸ¥æ˜¯å¦å·²ç»æ˜¯æ­£ç¡®çš„é“¾æ¥
    if href.startswith("https://byrutgame.org"):
        # å·²ç»æ˜¯æ­£ç¡®é“¾æ¥ï¼Œç›´æ¥ä½¿ç”¨
        proxy_url = href
    else:
        # ä¸æ˜¯æ­£ç¡®é“¾æ¥ï¼Œè½¬æ¢ä¸ºæ­£ç¡®é“¾æ¥
        detail_path = href.replace("https://napcat.1783069903.workers.dev", "")
        if not detail_path.startswith("/"):
            detail_path = "/" + detail_path
        proxy_url = f"https://byrutgame.org{detail_path}"
    
    # é‡è¯•æœºåˆ¶é…ç½®
    max_retries = 3
    retry_delay = 2
    html = None
    
    for attempt in range(max_retries):
        connector = aiohttp.TCPConnector(ssl=False, force_close=True)
        timeout = aiohttp.ClientTimeout(total=30, connect=10, sock_connect=10, sock_read=10)
        session = None
        
        try:
            # æ¯æ¬¡é‡è¯•éƒ½åˆ›å»ºå…¨æ–°çš„sessionå’Œconnector
            # ä¸ºäº†é¿å…403ï¼Œæˆ‘ä»¬ä¼ é€’ä¸€ä¸ªç©ºçš„cookieså­—å…¸ï¼Œè®©æœåŠ¡å™¨è®¤ä¸ºæˆ‘ä»¬æ¥å—cookies
            session = aiohttp.ClientSession(
                connector=connector, 
                timeout=timeout, 
                headers=HEADERS,
                cookies={}  # æ·»åŠ ç©ºcookiesï¼Œé¿å…æœåŠ¡å™¨æ‹’ç»æ— cookiesè¯·æ±‚
            )
            
            # æ„å»ºè¯·æ±‚å‚æ•°
            request_params = {"timeout": timeout}
            if PROXY:
                request_params["proxy"] = PROXY
            
            # æ‰“å°è°ƒè¯•æ—¥å¿—
            logging.debug(f"[Byrut] å‘é€è¯¦æƒ…é¡µè¯·æ±‚ï¼š{proxy_url}")
            logging.debug(f"[Byrut] è¯·æ±‚headersï¼š{HEADERS}")
            logging.debug(f"[Byrut] è¯·æ±‚proxyï¼š{PROXY}")
            
            async with session.get(proxy_url, **request_params) as resp:
                # æ‰“å°å“åº”æ—¥å¿—
                logging.debug(f"[Byrut] è¯¦æƒ…é¡µå“åº”çŠ¶æ€ç ï¼š{resp.status}")
                logging.debug(f"[Byrut] è¯¦æƒ…é¡µå“åº”headersï¼š{dict(resp.headers)}")
                
                if resp.status == 403:
                    logging.warning(f"[Byrut] è¯¦æƒ…é¡µ403 Forbidden (å°è¯• {attempt + 1}/{max_retries})")
                    # å°è¯•è·å–é”™è¯¯å“åº”å†…å®¹ï¼Œåˆ†æ403åŸå› 
                    error_content = await resp.text()
                    logging.debug(f"[Byrut] è¯¦æƒ…é¡µ403é”™è¯¯å“åº”å†…å®¹ï¼š{error_content[:500]}...")
                    
                    # 403æ—¶å°è¯•æ›´æ¢User-Agent
                    if attempt < max_retries - 1:
                        # è½®æ¢User-Agent
                        import random
                        user_agents = [
                            "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
                            "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/119.0.0.0 Safari/537.36",
                            "Mozilla/5.0 (Macintosh; Intel Mac OS X 14_0) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
                            "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Firefox/121.0",
                            "Mozilla/5.0 (Macintosh; Intel Mac OS X 14.1) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/17.1 Safari/605.1.15"
                        ]
                        # åˆ›å»ºä¸´æ—¶headerså‰¯æœ¬ï¼Œè½®æ¢User-Agent
                        temp_headers = HEADERS.copy()
                        temp_headers['User-Agent'] = random.choice(user_agents)
                        session.headers.update(temp_headers)
                        logging.debug(f"[Byrut] å°è¯•æ›´æ¢User-Agent: {temp_headers['User-Agent']}")
                        
                        await asyncio.sleep(retry_delay * (attempt + 1))  # æŒ‡æ•°é€€é¿
                        continue
                    # æœ€åä¸€æ¬¡å°è¯•å¤±è´¥ï¼Œä½¿ç”¨å¤‡ç”¨æ–¹æ¡ˆ
                    _apply_backup_solution(item, "HTTP 403 Forbiddené”™è¯¯")
                    return
                elif resp.status != 200:
                    logging.warning(f"[Byrut] è¯¦æƒ…é¡µçŠ¶æ€ç ï¼š{resp.status} (å°è¯• {attempt + 1}/{max_retries})")
                    # å°è¯•è·å–é”™è¯¯å“åº”å†…å®¹
                    error_content = await resp.text()
                    logging.debug(f"[Byrut] è¯¦æƒ…é¡µé”™è¯¯å“åº”å†…å®¹ï¼š{error_content[:500]}...")
                    if attempt < max_retries - 1:
                        await asyncio.sleep(retry_delay)
                        continue
                    # æœ€åä¸€æ¬¡å°è¯•å¤±è´¥ï¼Œä½¿ç”¨å¤‡ç”¨æ–¹æ¡ˆ
                    _apply_backup_solution(item, "HTTPçŠ¶æ€ç é”™è¯¯")
                    return
                html = await resp.text()
                logging.debug(f"[Byrut] æˆåŠŸè·å–è¯¦æƒ…é¡µå“åº”ï¼Œé•¿åº¦ï¼š{len(html)} å­—ç¬¦")
                break  # æˆåŠŸè·å–æ•°æ®ï¼Œè·³å‡ºé‡è¯•å¾ªç¯
                
        except aiohttp.ClientConnectorError as e:
            logging.error(f"[Byrut] è¯¦æƒ…é¡µè¿æ¥é”™è¯¯ (å°è¯• {attempt + 1}/{max_retries}): {e}")
            if attempt < max_retries - 1:
                await asyncio.sleep(retry_delay * (attempt + 1))  # æŒ‡æ•°é€€é¿
                continue
            # æœ€åä¸€æ¬¡å°è¯•å¤±è´¥ï¼Œä½¿ç”¨å¤‡ç”¨æ–¹æ¡ˆ
            _apply_backup_solution(item, "è¿æ¥é”™è¯¯")
            return
        except asyncio.TimeoutError as e:
            logging.error(f"[Byrut] è¯¦æƒ…é¡µè¯·æ±‚è¶…æ—¶ (å°è¯• {attempt + 1}/{max_retries}): {e}")
            if attempt < max_retries - 1:
                await asyncio.sleep(retry_delay)
                continue
            _apply_backup_solution(item, "è¯·æ±‚è¶…æ—¶")
            return
        except aiohttp.ClientPayloadError as e:
            logging.error(f"[Byrut] è¯¦æƒ…é¡µæ•°æ®ä¼ è¾“é”™è¯¯ (å°è¯• {attempt + 1}/{max_retries}): {e}")
            if attempt < max_retries - 1:
                await asyncio.sleep(retry_delay)
                continue
            _apply_backup_solution(item, "æ•°æ®ä¼ è¾“é”™è¯¯")
            return
        except Exception as e:
            logging.exception(f"[Byrut] è¯¦æƒ…é¡µè¯·æ±‚å¤±è´¥ (å°è¯• {attempt + 1}/{max_retries}): {e}")
            if attempt < max_retries - 1:
                await asyncio.sleep(retry_delay)
                continue
            _apply_backup_solution(item, "æœªçŸ¥é”™è¯¯")
            return
        finally:
            # ç¡®ä¿sessionè¢«æ­£ç¡®å…³é—­
            if session and not session.closed:
                await session.close()
            # ç¡®ä¿connectorè¢«æ­£ç¡®å…³é—­
            if connector and not connector.closed:
                await connector.close()
    
    # å¦‚æœæ²¡æœ‰è·å–åˆ°HTMLï¼Œä½¿ç”¨å¤‡ç”¨æ–¹æ¡ˆ
    if html is None:
        _apply_backup_solution(item, "æ— æ³•è·å–é¡µé¢å†…å®¹")
        return

    soup = BeautifulSoup(html, "html.parser")
    update_node = soup.select_one("div.tupd")
    update_text = update_node.get_text(strip=True) if update_node else ""
    m = re.search(r"(\d{1,2}\s+[Ğ°-ÑĞ-Ğ¯]+\s+\d{4},\s*\d{1,2}:\d{2})", update_text)
    
    if m:
        russian_date = m.group(1)
        # ä¿„æ–‡æœˆä»½æ˜ å°„
        month_map = {
            'ÑĞ½Ğ²Ğ°Ñ€Ñ': '1', 'Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ': '2', 'Ğ¼Ğ°Ñ€Ñ‚Ğ°': '3', 'Ğ°Ğ¿Ñ€ĞµĞ»Ñ': '4',
            'Ğ¼Ğ°Ñ': '5', 'Ğ¸ÑĞ½Ñ': '6', 'Ğ¸ÑĞ»Ñ': '7', 'Ğ°Ğ²Ğ³ÑƒÑÑ‚Ğ°': '8',
            'ÑĞµĞ½Ñ‚ÑĞ±Ñ€Ñ': '9', 'Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ': '10', 'Ğ½Ğ¾ÑĞ±Ñ€Ñ': '11', 'Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ': '12'
        }
        
        # è§£æä¿„æ–‡æ—¥æœŸ
        parts = russian_date.split()
        if len(parts) >= 3:
            day = parts[0]
            month_ru = parts[1].lower()
            year = parts[2].replace(',', '')
            
            # è½¬æ¢ä¸ºä¸­æ–‡æ ¼å¼
            if month_ru in month_map:
                month = month_map[month_ru]
                # æå–æ—¶é—´éƒ¨åˆ†
                time_match = re.search(r'(\d{1,2}:\d{2})', russian_date)
                time_str = time_match.group(1) if time_match else ""
                
                # æ ¼å¼åŒ–ä¸ºä¸­æ–‡æ—¥æœŸæ ¼å¼
                update_time = f"{year}-{month}-{day} {time_str}".strip()
            else:
                update_time = russian_date  # å¦‚æœè½¬æ¢å¤±è´¥ï¼Œä¿æŒåŸæ ·
        else:
            update_time = russian_date
    else:
        update_time = "æœªçŸ¥"

    tor_tag = soup.select_one("a.itemtop_games") or soup.select_one("a:-soup-contains('Ğ¡ĞºĞ°Ñ‡Ğ°Ñ‚ÑŒ Ñ‚Ğ¾Ñ€Ñ€ĞµĞ½Ñ‚')")
    torrent_url = tor_tag["href"] if tor_tag else None
    if torrent_url and torrent_url.startswith("/"):
        torrent_url = f"https://byrutgame.org{torrent_url}"

    item.update({"update_time": update_time, "torrent_url": torrent_url})


def image_to_base64(image_path):
    """å°†å›¾ç‰‡æ–‡ä»¶è½¬æ¢ä¸ºbase64ç¼–ç å­—ç¬¦ä¸²"""
    try:
        if not os.path.exists(image_path):
            logging.warning(f"å›¾ç‰‡æ–‡ä»¶ä¸å­˜åœ¨: {image_path}")
            return None
        
        with open(image_path, 'rb') as f:
            image_data = f.read()
            base64_encoded = base64.b64encode(image_data).decode('utf-8')
            return f"data:image/png;base64,{base64_encoded}"
    except Exception as e:
        logging.error(f"å›¾ç‰‡è½¬base64å¤±è´¥: {e}")
        return None

async def send_final_forward(group_id, èµåŠ©å†…å®¹: list[str], å•æœº_lines: list[str], è”æœº_lines: list[str], user_id: str = "0", user_nickname: str = "æ¸¸æˆåŠ©æ‰‹"):
    """ä¸€æ¬¡æ€§æ„é€ ï¼šèµåŠ© + å•æœºç‰ˆ + è”æœºç‰ˆï¼ˆèŠ‚ç‚¹å†…ä¸å†å†™æ¸¸æˆåï¼‰"""
    nodes = []

    # 1. èµåŠ©èŠ‚ç‚¹
    # ä½¿ç”¨ base64 ç¼–ç çš„å›¾ç‰‡
    base_dir = "/home/hjh/BOT/NCBOT"
    abs_qq_img_path = QQ_IMG
    qq_img_base64 = image_to_base64(abs_qq_img_path)
    
    sponsor_content = [{"type": "text", "data": {"text": èµåŠ©å†…å®¹[0]}}]
    if qq_img_base64:
        sponsor_content.append({"type": "image", "data": {"file": qq_img_base64}})
    
    # ä»æ¶ˆæ¯ä¸­æå–æ¸¸æˆåç§°ï¼Œç”¨äºæ ‡é¢˜å’Œæ‘˜è¦
    game_title = ""
    for line in å•æœº_lines:
        if "æ¸¸æˆåå­—" in line:
            game_title = line.split("æ¸¸æˆåå­—ï¼š")[1].strip()
            break
    if not game_title:
        for line in è”æœº_lines:
            if "æ¸¸æˆåå­—" in line:
                game_title = line.split("æ¸¸æˆåå­—ï¼š")[1].strip()
                break
    if not game_title:
        game_title = "æ¸¸æˆèµ„æº"

    # 1. èµåŠ©èŠ‚ç‚¹
    nodes.append({
        "type": "node",
        "data": {
            "uin": user_id,
            "nickname": user_nickname,
            "content": sponsor_content
        }
    })

    # 2. å•æœºç‰ˆèŠ‚ç‚¹ï¼ˆå»æ‰æ ‡é¢˜è¡Œï¼Œåªå†™ç½‘ç›˜ä¿¡æ¯ï¼‰
    å•æœº_nodes = [{"type": "text", "data": {"text": line}} for line in å•æœº_lines]
    nodes.append({
        "type": "node",
        "data": {
            "uin": user_id,
            "nickname": user_nickname,
            "content": å•æœº_nodes
        }
    })

    # 3. è”æœºç‰ˆèŠ‚ç‚¹ï¼ˆç›´æ¥ä½¿ç”¨å¤„ç†å¥½çš„å†…å®¹ï¼Œä¸å†é‡å¤æ·»åŠ æ ‡é¢˜ï¼‰
    è”æœº_nodes = []
    # ç›´æ¥è¿½åŠ å¤„ç†å¥½çš„å†…å®¹
    if è”æœº_lines:
        è”æœº_nodes.extend([{"type": "text", "data": {"text": line}} for line in è”æœº_lines])
        
        # â‘¢ æ£€æŸ¥æ˜¯å¦æœ‰å¤‡ç”¨å›¾ç‰‡éœ€è¦æ·»åŠ 
        for line in è”æœº_lines:
            if "å¤‡ç”¨å›¾ç‰‡" in line and line.split("å¤‡ç”¨å›¾ç‰‡ï¼š")[1].strip():
                image_path = line.split("å¤‡ç”¨å›¾ç‰‡ï¼š")[1].strip()
                if os.path.exists(image_path):
                    # ä½¿ç”¨ base64 ç¼–ç çš„å›¾ç‰‡
                    if not os.path.isabs(image_path):
                        base_dir = "/home/hjh/BOT/NCBOT"
                        abs_image_path = os.path.join(base_dir, "tool", os.path.basename(image_path))
                    else:
                        abs_image_path = image_path
                    
                    # è½¬æ¢ä¸º base64
                    backup_img_base64 = image_to_base64(abs_image_path)
                    if backup_img_base64:
                        è”æœº_nodes.append({"type": "image", "data": {"file": backup_img_base64}})
                    break
    nodes.append({
        "type": "node",
        "data": {
            "uin": user_id,
            "nickname": user_nickname,
            "content": è”æœº_nodes
        }
    })

    # 4. ä¸€æ¬¡æ€§å‘å‡º
    url = "http://101.35.164.122:3006/send_group_forward_msg"
    headers = {'Content-Type': 'application/json', 'Authorization': 'Bearer he031701'}
    
    # è®¡ç®—èµ„æºæ•°é‡
    single_count = len([line for line in å•æœº_lines if "é“¾æ¥" in line])
    multi_count = len([line for line in è”æœº_lines if "ç§å­é“¾æ¥" in line])
    total_count = single_count + multi_count
    
    summary = f"å…±æ‰¾åˆ° {total_count} ä¸ªèµ„æºé“¾æ¥"
    if single_count > 0:
        summary += f" (å•æœº: {single_count} ä¸ª)"
    if multi_count > 0:
        summary += f" (è”æœº: {multi_count} ä¸ª)"
    
    payload = {
        "group_id": group_id,
        "messages": nodes,
        "source": game_title,
        "summary": summary,
        "prompt": f"[{game_title}]",
        "news": [{"text": "ç‚¹å‡»æŸ¥çœ‹æ¸¸æˆèµ„æºè¯¦æƒ…"}]
    }

    # 5. å¢å¼ºé”™è¯¯å¤„ç†å’Œç½‘ç»œå®¹é”™
    max_retries = 3
    retry_delay = 2
    
    for attempt in range(max_retries):
        try:
            async with aiohttp.ClientSession(timeout=aiohttp.ClientTimeout(total=30)) as session:
                async with session.post(url, json=payload, headers=headers) as resp:
                    if resp.status == 200:
                        result = await resp.json()
                        if result.get('status') == 'ok':
                            logging.info(f"[Byrut] è½¬å‘æ¶ˆæ¯å‘é€æˆåŠŸ")
                            return True
                        else:
                            logging.warning(f"[Byrut] è½¬å‘æ¶ˆæ¯å‘é€å¤±è´¥: {result}")
                    else:
                        logging.warning(f"[Byrut] HTTPçŠ¶æ€ç é”™è¯¯: {resp.status}")
                        
                    if attempt < max_retries - 1:
                        logging.info(f"[Byrut] é‡è¯•å‘é€è½¬å‘æ¶ˆæ¯ (å°è¯• {attempt + 2}/{max_retries})")
                        await asyncio.sleep(retry_delay)
                        continue
                    else:
                        logging.error(f"[Byrut] è½¬å‘æ¶ˆæ¯å‘é€å¤±è´¥ï¼Œå·²è¾¾åˆ°æœ€å¤§é‡è¯•æ¬¡æ•°")
                        return False
                        
        except asyncio.TimeoutError as e:
            logging.error(f"[Byrut] è½¬å‘æ¶ˆæ¯å‘é€è¶…æ—¶ (å°è¯• {attempt + 1}/{max_retries}): {e}")
            if attempt < max_retries - 1:
                await asyncio.sleep(retry_delay)
                continue
        except aiohttp.ClientConnectorError as e:
            logging.error(f"[Byrut] è½¬å‘æ¶ˆæ¯è¿æ¥é”™è¯¯ (å°è¯• {attempt + 1}/{max_retries}): {e}")
            if attempt < max_retries - 1:
                await asyncio.sleep(retry_delay * (attempt + 1))  # æŒ‡æ•°é€€é¿
                continue
        except Exception as e:
            logging.exception(f"[Byrut] è½¬å‘æ¶ˆæ¯å‘é€å¼‚å¸¸ (å°è¯• {attempt + 1}/{max_retries}): {e}")
            if attempt < max_retries - 1:
                await asyncio.sleep(retry_delay)
                continue
    
    return False


# -------------------- æ’ä»¶ä¸»ç±» --------------------
class Xydj(BasePlugin):
    name = "xydj"
    version = "1.0"

    def __init__(self, *args, **kwargs):
        super().__init__(*args, **kwargs)
        self.message_queue = asyncio.Queue()
        self.waiting_for_reply = False
        self.processing = False
        self.user_who_sent_command = None
        self.filtered_games = []
        self.timer_task = None
        self._cache = load_cache()

    async def countdown(self, msg, group_id):
        await asyncio.sleep(40)
        if self.waiting_for_reply:
            self._cleanup()
            await self.api.post_group_msg(
                group_id=group_id,
                rtf=MessageChain([Reply(msg.message_id), Text("ç­‰å¾…è¶…æ—¶ï¼Œæ“ä½œå·²å–æ¶ˆã€‚è¯·é‡æ–°æœç´¢")])
            )

    def _cleanup(self):
        self.waiting_for_reply = False
        self.processing = False
        if self.timer_task:
            self.timer_task.cancel()
            self.timer_task = None

    async def process_game_resource(self, game, msg):
        """ç»Ÿä¸€å¤„ç†æ¸¸æˆèµ„æºè·å–å’Œå‘é€çš„å‡½æ•°ï¼ˆå¹¶è¡Œå¤„ç†å•æœºç‰ˆå’Œè”æœºç‰ˆèµ„æºï¼‰"""
        try:
            # è·å–å¤„ç†åçš„åå­—å’Œä¸­æ–‡å±•ç¤ºå
            english_keyword, chinese_display = extract_english_name(game['title'])
            # æ‰“å°æœç´¢ç”¨çš„è‹±æ–‡ååˆ°æ§åˆ¶å°
            print(f"[æœç´¢å…³é”®è¯] ä¸­æ–‡å: {chinese_display}, è‹±æ–‡å: {english_keyword}")

            # å¹¶è¡Œå¤„ç†å•æœºç‰ˆå’Œè”æœºç‰ˆèµ„æº
            async def process_single_player():
                """å¤„ç†å•æœºç‰ˆèµ„æº"""
                å•æœºå†…å®¹ = []
                å•æœº_lines = await extract_download_info(game['url'])
                if å•æœº_lines:
                    å•æœºå†…å®¹.append("ğŸ® ã€å•æœºç‰ˆã€‘\n")
                    å•æœºå†…å®¹.append(f"ğŸ“Œ æ¸¸æˆåå­—ï¼š{chinese_display}\n")   # â† ä¸­æ–‡å±•ç¤ºå
                    # é€è¡ŒåŠ  \n ä¿è¯å¯†ç /é“¾æ¥åéƒ½æ¢è¡Œ
                    for line in å•æœº_lines:
                        if "è§£å‹å¯†ç " in line:
                            å•æœºå†…å®¹.append(f"ğŸ”‘ {line}\n")
                        elif "ç™¾åº¦ç½‘ç›˜" in line:
                            å•æœºå†…å®¹.append(f"ğŸ’¾ {line}\n")
                        elif "é“¾æ¥" in line:
                            å•æœºå†…å®¹.append(f"ğŸŒ {line}\n")
                        else:
                            å•æœºå†…å®¹.append(f"ğŸ“‹ {line}\n")
                else:
                    å•æœºå†…å®¹.append("ğŸ® ã€å•æœºç‰ˆã€‘\n")
                    å•æœºå†…å®¹.append("âŒ æœªæ‰¾åˆ°ç›¸å…³èµ„æº\n")
                return å•æœºå†…å®¹

            async def process_multi_player():
                """å¤„ç†è”æœºç‰ˆèµ„æº"""
                # Byrut è”æœºç‰ˆï¼ˆç”¨è‹±æ–‡å…³é”®è¯æœï¼Œå±•ç¤ºç”¨å®Œæ•´æ ‡é¢˜ï¼‰
                byrut_results = await search_byrut(english_keyword)   # æœç´¢ä»èµ°è‹±æ–‡
                # æ‰“å°æœç´¢åˆ°çš„hrefåˆ°æ§åˆ¶å°
                for item in byrut_results:
                    print(f"[Byrut] æ‰¾åˆ°è”æœºèµ„æº: {item['href']}")
                    await fetch_byrut_detail(item)
                
                # è”æœºç‰ˆå†…å®¹ï¼ˆè‹±æ–‡å±•ç¤ºå + æ›´æ–°æ—¶é—´ + ç§å­ï¼‰
                è”æœºå†…å®¹ = []
                if byrut_results:
                    è”æœºå†…å®¹.append("ğŸ® ã€è”æœºç‰ˆã€‘\n")
                    è”æœºå†…å®¹.append(f"ğŸ“Œ æ¸¸æˆåå­—ï¼š{english_keyword}\n")   # â† è‹±æ–‡å±•ç¤ºå
                    
                    for idx, item in enumerate(byrut_results, 1):
                        if len(byrut_results) > 1:
                            è”æœºå†…å®¹.append(f"\n{idx}. èµ„æº {idx}\n")
                        
                        è”æœºå†…å®¹.append(f"ğŸ”‘ è§£å‹å¯†ç ï¼šã€online-fix.meã€‘\n")
                        è”æœºå†…å®¹.append(f"â° æ›´æ–°æ—¶é—´ï¼š{item['update_time']}\n")
                        
                        if item.get('torrent_url'):
                            è”æœºå†…å®¹.append(f"ğŸŒ ç§å­é“¾æ¥ï¼š{item['torrent_url']}\n")
                        else:
                            è”æœºå†…å®¹.append(f"âŒ ç§å­é“¾æ¥ï¼šæš‚æ— \n")
                        
                        # å¦‚æœæœ‰å¤‡ç”¨å›¾ç‰‡ï¼Œæ·»åŠ å›¾ç‰‡æ ‡è®°
                        if item.get('backup_image'):
                            è”æœºå†…å®¹.append(f"ğŸ–¼ï¸ å¤‡ç”¨å›¾ç‰‡ï¼š{item['backup_image']}\n")
                    
                    è”æœºå†…å®¹.append("ğŸ’¡ ä½¿ç”¨æç¤ºï¼šä¸‹è½½ç§å­åä½¿ç”¨BTå®¢æˆ·ç«¯æ‰“å¼€å³å¯\n")
                else:
                    è”æœºå†…å®¹.append("ğŸ® ã€è”æœºç‰ˆã€‘\n")
                    è”æœºå†…å®¹.append("âŒ æœªæ‰¾åˆ°ç›¸å…³èµ„æº\n")
                    è”æœºå†…å®¹.append("ğŸ”‘ é€šç”¨è§£å‹å¯†ç ï¼šã€online-fix.meã€‘\n")
                    è”æœºå†…å®¹.append("ğŸ“š æŸ¥çœ‹æ•™ç¨‹ï¼šã€Šæœç´¢å’Œä½¿ç”¨è”æœºæ¸¸æˆã€‹\n")
                    è”æœºå†…å®¹.append("ğŸŒ https://www.yuque.com/lanmeng-ijygo/ey7ah4/fe9hfep86cw7coku?singleDoc#\n")
                return è”æœºå†…å®¹

            # å¹¶è¡Œæ‰§è¡Œå•æœºç‰ˆå’Œè”æœºç‰ˆèµ„æºè·å–
            å•æœºå†…å®¹, è”æœºå†…å®¹ = await asyncio.gather(
                process_single_player(),
                process_multi_player(),
                return_exceptions=True  # æ•è·å¼‚å¸¸ï¼Œç¡®ä¿ä¸€ä¸ªä»»åŠ¡å¤±è´¥ä¸ä¼šå½±å“å¦ä¸€ä¸ª
            )
            
            # å¤„ç†å¯èƒ½çš„å¼‚å¸¸
            if isinstance(å•æœºå†…å®¹, Exception):
                print(f"å•æœºç‰ˆèµ„æºè·å–å¤±è´¥: {å•æœºå†…å®¹}")
                å•æœºå†…å®¹ = ["ã€å•æœºç‰ˆã€‘è·å–èµ„æºæ—¶å‡ºé”™\n"]
            
            if isinstance(è”æœºå†…å®¹, Exception):
                print(f"è”æœºç‰ˆèµ„æºè·å–å¤±è´¥: {è”æœºå†…å®¹}")
                è”æœºå†…å®¹ = ["ã€è”æœºç‰ˆã€‘è·å–èµ„æºæ—¶å‡ºé”™"]
            
            # 4. ä¸€æ¬¡æ€§è½¬å‘
            èµåŠ©å†…å®¹ = ["æ­£è§„æµé‡å¡ä¸æ˜¯ç‰©è”å¡ï¼Œå®˜æ–¹å®¢æœå¯æŸ¥å¥—é¤ï¼Œå®˜æ–¹APPå¯è‡ªå·±æŸ¥ä½™é¢\næœ‰é—®é¢˜è¯·æ‰£ä¸»äºº~~ï¼š\nhttps://ym.ksjhaoka.com/?s=q9thdGIs326398"]
            
            # å¦‚æœä¸¤æ¡éƒ½ç©ºï¼Œå†æç¤ºã€Œéƒ¨åˆ†æœªæ‰¾åˆ°ã€
            if not å•æœºå†…å®¹ and not è”æœºå†…å®¹:
                await self.api.post_group_msg(
                    group_id=msg.group_id,
                    rtf=MessageChain([Reply(msg.message_id), Text("ã€è”æœºç‰ˆã€‘æœªæ‰¾åˆ°ä»»ä½•èµ„æºï¼Œå¯èƒ½å…³é”®è¯ä¸åŒ¹é…æˆ–æœåŠ¡å™¨å¼‚å¸¸")])
                )
                return
            
            # å¦åˆ™ã€Œæœ‰å¤šå°‘å‘å¤šå°‘ã€
            await send_final_forward(msg.group_id, èµåŠ©å†…å®¹, å•æœºå†…å®¹, è”æœºå†…å®¹, str(msg.user_id), msg.sender.nickname)
        except Exception as e:
            await self.api.post_group_msg(
                group_id=msg.group_id, rtf=MessageChain([Reply(msg.message_id), Text(f"å¤„ç†å¤±è´¥: {str(e)}")])
            )

    async def process_single_game(self, game, msg):
        """å¤„ç†å•ä¸ªæ¸¸æˆçš„è‡ªåŠ¨è½¬å‘"""
        await self.process_game_resource(game, msg)

    @bot.group_event
    async def on_group_message(self, msg: GroupMessage):
        await self.message_queue.put(msg)
        if self.waiting_for_reply and msg.user_id == self.user_who_sent_command:
            if self.processing:
                return
            choice = re.sub(r'\[CQ:[^\]]+\]', '', msg.raw_message).strip()
            if choice == "0":
                await self.api.post_group_msg(
                    group_id=msg.group_id, rtf=MessageChain([Reply(msg.message_id), Text("æ“ä½œå·²å–æ¶ˆã€‚")])
                )
                self._cleanup()
                return
            if not choice.isdigit() or not 1 <= int(choice) <= len(self.filtered_games):
                await self.api.post_group_msg(
                    group_id=msg.group_id, rtf=MessageChain([Reply(msg.message_id), Text("å›å¤é”™è¯¯ï¼Œæ“ä½œå·²å–æ¶ˆã€‚è¯·é‡æ–°æœç´¢æ¸¸æˆã€‚")])
                )
                self._cleanup()
                return
            choice = int(choice)
            await self.api.post_group_msg(
                group_id=msg.group_id, rtf=MessageChain([Reply(msg.message_id), Text(f"å·²é€‰æ‹©ç¬¬ {choice} ä¸ªæ¸¸æˆï¼Œè¯·ç­‰å¾…å¤§æ¦‚1åˆ†é’Ÿï¼ï¼ï¼")])
            )
            self.processing = True
            self._cleanup()
            try:
                game = self.filtered_games[choice - 1]
                await self.process_game_resource(game, msg)
            except Exception as e:
                await self.api.post_group_msg(
                    group_id=msg.group_id, rtf=MessageChain([Reply(msg.message_id), Text(f"å¤„ç†å¤±è´¥: {str(e)}")])
                )
            finally:
                self._cleanup()
        elif msg.raw_message.strip().startswith("æœç´¢"):
            game_name = msg.raw_message.strip()[2:].strip()
            if not game_name:
                await self.api.post_group_msg(
                    group_id=msg.group_id, rtf=MessageChain([Reply(msg.message_id), Text("ä½¿ç”¨æ–¹æ³•ï¼šæœç´¢+æ¸¸æˆåç§°ï¼Œä¾‹å¦‚ï¼šæœç´¢ æ–‡æ˜6")])
                )
                return
            try:
                text_result, games = await search_game(game_name)
                if not text_result:
                    await self.api.post_group_msg(
                        group_id=msg.group_id, rtf=MessageChain([Reply(msg.message_id), Text("æœªæ‰¾åˆ°ï¼Œæ£€æŸ¥æ¸¸æˆåå­—ï¼Œæœç´¢æ¸¸æˆå­—æ•°å°‘ä¸€ç‚¹è¯•è¯•å‘¢")])
                    )
                    return
                
                # å¦‚æœåªæœ‰ä¸€ä¸ªæ¸¸æˆç»“æœï¼Œç›´æ¥è‡ªåŠ¨å¤„ç†
                if len(games) == 1:
                    await self.api.post_group_msg(
                        group_id=msg.group_id, rtf=MessageChain([Reply(msg.message_id), Text("æœç´¢åˆ°1ä¸ªæ¸¸æˆï¼Œè‡ªåŠ¨ä¸ºæ‚¨è·å–èµ„æºä¿¡æ¯ï¼Œè¯·ç­‰å¾…å¤§æ¦‚1åˆ†é’Ÿï¼ï¼ï¼")])
                    )
                    # ç›´æ¥å¤„ç†å•ä¸ªæ¸¸æˆ
                    await self.process_single_game(games[0], msg)
                    return
                
                # å¤šä¸ªæ¸¸æˆç»“æœï¼Œéœ€è¦ç”¨æˆ·é€‰æ‹©ï¼ˆç›´æ¥å‘é€æ–‡æœ¬ï¼Œä¸å‘é€å›¾ç‰‡ï¼‰
                await self.api.post_group_msg(
                    group_id=msg.group_id, rtf=MessageChain([Reply(msg.message_id), Text(f"ğŸ¯ å‘ç° {len(games)} æ¬¾æ¸¸æˆ\n{text_result}\nâ° 30ç§’å†…å›å¤åºå·é€‰æ‹© | å›å¤ 0 å–æ¶ˆæ“ä½œ")])
                )
                self.waiting_for_reply = True
                self.user_who_sent_command = msg.user_id
                self.filtered_games = games
                self.timer_task = asyncio.create_task(self.countdown(msg, msg.group_id))
            except Exception as e:
                await self.api.post_group_msg(
                    group_id=msg.group_id, rtf=MessageChain([Reply(msg.message_id), Text("å‘ç”Ÿé”™è¯¯ï¼Œè¯·ç¨åé‡è¯•ã€‚")])
                )

    async def on_load(self):
        print(f"{self.name} æ’ä»¶å·²åŠ è½½ï¼Œç‰ˆæœ¬: {self.version}")